{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nirmal Budhathoki- DSE220 Final_ Selected Submission 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import math\n",
    "import string\n",
    "import sklearn.metrics as skmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import ensemble\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read from gzip file\n",
    "def read_gzip(filename):\n",
    "    for line in gzip.open(filename):\n",
    "        yield eval(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get (positive/negative) opinion words from corpus\n",
    "# I downloaded .txt files for positive and negative words from http://positivewordsresearch.com/sentiment-analysis-resources/\n",
    "\n",
    "def retrieve_words(filename):\n",
    "    with open(filename,'r',encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            yield line\n",
    "            \n",
    "positive_words = []\n",
    "negative_words = []\n",
    "\n",
    "for pword in retrieve_words('positive-words.txt'): # collecting positive words in the list\n",
    "    positive_words.append(pword[:-1])\n",
    "    \n",
    "for nword in retrieve_words('negative-words.txt'): # collecting negative words in the list\n",
    "    negative_words.append(nword[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a+', 'abound', 'abounds', 'abundance', 'abundant']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2-faced', '2-faces', 'abnormal', 'abolish', 'abominable']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting dataset based on number of votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the long tail problem with the dataset, I splitted the data between two sets one for higher votes and one for lower number of votes, and used ensembling models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data with high votes\n",
    "high_vote_dataset = []\n",
    "for line in read_gzip(\"train.json.gz\"):\n",
    "    if (line['helpful'])['outOf'] >15 and (line['helpful'])['outOf'] <150: #during EDA 15 is the best boundary number noticed\n",
    "        high_vote_dataset.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2767"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(high_vote_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data with low votes, leaving outOf= 0 votes out\n",
    "low_vote_dataset = []\n",
    "for line in read_gzip(\"train.json.gz\"):\n",
    "    if (line['helpful'])['outOf'] <= 40  and (line['helpful'])['outOf'] > 1 : #again EDA gave me insight for boundary numbers\n",
    "        low_vote_dataset.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33876"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(low_vote_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the rating\n",
    "def get_rating(review_data):\n",
    "    ratings = []\n",
    "    for x in review_data:\n",
    "        ratings.append(x['rating'])\n",
    "    return ratings\n",
    "\n",
    "#get helpful votes\n",
    "def get_helpful_votes(review_data):\n",
    "    helpfulness_votes = []\n",
    "    for x in review_data:\n",
    "        votes = x['helpful']\n",
    "        helpfulness_votes.append(np.log(votes['outOf'] + 1))\n",
    "    return helpfulness_votes\n",
    "\n",
    "#get word count in review\n",
    "# punctuation = set(string.punctuation)\n",
    "# from collections import defaultdict\n",
    "\n",
    "#get word count in review\n",
    "def get_reviewText_word_count(review_data):\n",
    "    data_review_word_count = []\n",
    "    for x in review_data:\n",
    "        data_review_word_count.append(np.log(len(x['reviewText'].lower().split())+1))\n",
    "    return data_review_word_count\n",
    "    \n",
    "#get allcaps words in reviewText\n",
    "def get_reviewText_allcaps_count(review_data):\n",
    "    data_review_word_allcaps_count = []\n",
    "    for x in review_data:\n",
    "        data_review_word_allcaps_count.append(np.log(len([word for word in x['reviewText'].split() if word.isupper()])+1))\n",
    "    return data_review_word_allcaps_count\n",
    "\n",
    "def get_reviewText_specialchar_count(dataset):\n",
    "    spec_chars=['!','#','$','%','^','&','*','?']\n",
    "    data_review_specialchar_count = []\n",
    "    for data_point in dataset:\n",
    "        data_review_specialchar_count.append(np.log(len([word for word in data_point['reviewText'].lower().split() \n",
    "                                           if word in spec_chars])+1))\n",
    "    return data_review_specialchar_count\n",
    "\n",
    "#get flesch reading ease score of review text\n",
    "from textstat.textstat import textstat\n",
    "def get_flesch_reading_ease_score(review_data):\n",
    "    data_review_flesch_reading_score = []\n",
    "    for x in review_data:\n",
    "        data_review_flesch_reading_score.append(textstat.flesch_reading_ease(x['reviewText']))\n",
    "    return data_review_flesch_reading_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#one hot encoding for category\n",
    "def get_category_id(review_data):\n",
    "    cat_0 = []\n",
    "    cat_1 = []\n",
    "    cat_2 = []\n",
    "    cat_3 = []\n",
    "    cat_4 = []\n",
    "    \n",
    "    for x in review_data:\n",
    "        cat_id = x['categoryID']\n",
    "        if cat_id == 0:\n",
    "            cat_0.append(1)\n",
    "            cat_1.append(0)\n",
    "            cat_2.append(0)\n",
    "            cat_3.append(0)\n",
    "            cat_4.append(0)\n",
    "        if cat_id == 1:\n",
    "            cat_0.append(0)\n",
    "            cat_1.append(1)\n",
    "            cat_2.append(0)\n",
    "            cat_3.append(0)\n",
    "            cat_4.append(0)\n",
    "        if cat_id == 2:\n",
    "            cat_0.append(0)\n",
    "            cat_1.append(0)\n",
    "            cat_2.append(1)\n",
    "            cat_3.append(0)\n",
    "            cat_4.append(0)\n",
    "        if cat_id == 3:\n",
    "            cat_0.append(0)\n",
    "            cat_1.append(0)\n",
    "            cat_2.append(0)\n",
    "            cat_3.append(1)\n",
    "            cat_4.append(0)\n",
    "        if cat_id == 4:\n",
    "            cat_0.append(0)\n",
    "            cat_1.append(0)\n",
    "            cat_2.append(0)\n",
    "            cat_3.append(0)\n",
    "            cat_4.append(1)\n",
    "    return cat_0, cat_1, cat_2, cat_3, cat_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_reviewText_posneg_diff_words(review_data):\n",
    "    review_posneg_diff_words = []\n",
    "    for x in review_data:\n",
    "        review = x['reviewText'].lower().split()\n",
    "        neg = sum([1 if word in negative_words else 0 for word in review])\n",
    "        pos = sum([1 if word in positive_words else 0 for word in review])\n",
    "        review_posneg_diff_words.append(np.log(abs(pos-neg)+1))\n",
    "    return review_posneg_diff_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def collect_features(review_data):\n",
    "\n",
    "    ratings = get_rating(review_data)\n",
    "    \n",
    "    helpful_votes = get_helpful_votes(review_data)\n",
    "    \n",
    "    review_word_count = get_reviewText_word_count(review_data)\n",
    "    \n",
    "    review_word_allcaps_count = get_reviewText_allcaps_count(review_data)\n",
    "    \n",
    "    review_specialchar_count = get_reviewText_specialchar_count(review_data)\n",
    "    \n",
    "    ease_of_reading_score=get_flesch_reading_ease_score(review_data)\n",
    "    \n",
    "    cat_0, cat_1, cat_2, cat_3, cat_4 = get_category_id(review_data)\n",
    "    \n",
    "    review_posneg_diff = get_reviewText_posneg_diff_words(review_data)\n",
    "    \n",
    "    feature_set = [np.ones(len(review_data)),ratings,helpful_votes,review_word_count,review_word_allcaps_count,\\\n",
    "                   review_specialchar_count,ease_of_reading_score,cat_0,cat_1, cat_2, cat_3,\\\n",
    "                   cat_4,review_posneg_diff]\n",
    "    \n",
    "    review_data = np.stack(feature_set, axis=1)\n",
    "    print (\"All Features are successfully extracted...\")\n",
    "    return review_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted helpfulness score for 2767 rows of data\n"
     ]
    }
   ],
   "source": [
    "train_high_helpfulness = []\n",
    "for x in high_vote_dataset:\n",
    "    data_helpfulness = x['helpful']\n",
    "    train_high_helpfulness.append(data_helpfulness['nHelpful'] * 1.0/data_helpfulness['outOf'])\n",
    "\n",
    "train_high_helpfulness = np.matrix(train_high_helpfulness).T\n",
    "print (\"Extracted helpfulness score for \" + str(len(train_high_helpfulness)) + \" rows of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted helpfulness score for 33876 rows of data\n"
     ]
    }
   ],
   "source": [
    "train_low_helpfulness = []\n",
    "for x in low_vote_dataset:\n",
    "    data_helpfulness = x['helpful']\n",
    "    train_low_helpfulness.append(data_helpfulness['nHelpful'] * 1.0/data_helpfulness['outOf'])\n",
    "\n",
    "train_low_helpfulness = np.matrix(train_low_helpfulness).T\n",
    "print (\"Extracted helpfulness score for \" + str(len(train_low_helpfulness)) + \" rows of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Features are successfully extracted...\n"
     ]
    }
   ],
   "source": [
    "#Collecting features for high votes data\n",
    "train_high_dataset = collect_features(high_vote_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Features are successfully extracted...\n"
     ]
    }
   ],
   "source": [
    "#collecting features for low votes data\n",
    "train_low_dataset = collect_features(low_vote_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error of Predictor : 0.0689207493827\n",
      "Mean Absolute Error of Predictor : 0.0780129975504\n",
      "Mean Absolute Error of Predictor : 0.0768868528261\n",
      "Mean Absolute Error of Predictor : 0.0706008441347\n",
      "Mean Absolute Error of Predictor : 0.0692037430554\n",
      "Mean Absolute Error of Predictor : 0.0816918792378\n",
      "Mean Absolute Error of Predictor : 0.0765967238911\n",
      "Mean Absolute Error of Predictor : 0.0713873756136\n",
      "Mean Absolute Error of Predictor : 0.0717808813845\n",
      "Mean Absolute Error of Predictor : 0.0734490812551\n"
     ]
    }
   ],
   "source": [
    "#Cross Validation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "kf=KFold(n_splits=10, random_state=None, shuffle=True)\n",
    "for train_index, test_index in kf.split(train_high_dataset):\n",
    "#print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#Fitting model\n",
    "    model_1=ElasticNet(alpha=0.09, l1_ratio=0.005)\n",
    "    model_1.fit(train_high_dataset[train_index], train_high_helpfulness[train_index])\n",
    "#predict    \n",
    "    pred_1=model_1.predict(train_high_dataset[test_index])\n",
    "#mae\n",
    "    mae_1 = mae(train_high_helpfulness[test_index], pred_1)\n",
    "    print (\"Mean Absolute Error of Predictor : \" + str(mae_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error of Predictor : 0.191622257551\n",
      "Mean Absolute Error of Predictor : 0.192149175583\n",
      "Mean Absolute Error of Predictor : 0.196824634686\n",
      "Mean Absolute Error of Predictor : 0.192415327003\n",
      "Mean Absolute Error of Predictor : 0.19371851015\n",
      "Mean Absolute Error of Predictor : 0.191916719755\n",
      "Mean Absolute Error of Predictor : 0.191900746561\n",
      "Mean Absolute Error of Predictor : 0.194314356039\n",
      "Mean Absolute Error of Predictor : 0.194778132462\n",
      "Mean Absolute Error of Predictor : 0.191928261529\n"
     ]
    }
   ],
   "source": [
    "kf=KFold(n_splits=10, random_state=None, shuffle=True)\n",
    "for train_index, test_index in kf.split(train_low_dataset):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#Fitting model\n",
    "    model_2=ensemble.GradientBoostingRegressor(n_estimators= 220, max_depth= 4, min_samples_split= 2, loss= 'ls')\n",
    "    model_2.fit(train_low_dataset[train_index], train_low_helpfulness[train_index].A1)\n",
    "#predict    \n",
    "    pred_2=model_2.predict(train_low_dataset[test_index])\n",
    "#mae\n",
    "    mae_2 = mae(train_low_helpfulness[test_index], pred_2)\n",
    "    print (\"Mean Absolute Error of Predictor : \" + str(mae_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_high_x,valid_high_x,train_high_y,valid_high_y=train_test_split(train_high_dataset,\\\n",
    "                                        train_high_helpfulness, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1936, 13)\n",
      "(831, 13)\n",
      "(1936, 1)\n",
      "(831, 1)\n"
     ]
    }
   ],
   "source": [
    "print (train_high_x.shape)\n",
    "print (valid_high_x.shape)\n",
    "print (train_high_y.shape)\n",
    "print (valid_high_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_low_x,valid_low_x,train_low_y,valid_low_y=train_test_split(train_low_dataset,\\\n",
    "                                        train_low_helpfulness, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23713, 13)\n",
      "(10163, 13)\n",
      "(23713, 1)\n",
      "(10163, 1)\n"
     ]
    }
   ],
   "source": [
    "print (train_low_x.shape)\n",
    "print (valid_low_x.shape)\n",
    "print (train_low_y.shape)\n",
    "print (valid_low_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Parameter Tuning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33876, 1)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_low_dataset.shape\n",
    "train_low_helpfulness.reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'learning_rate': 0.05,\n",
       "  'max_depth': 6,\n",
       "  'max_features': 0.3,\n",
       "  'min_samples_leaf': 9},\n",
       " -0.1948817182989291)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#VALIDATION DATA TO TUNE PARAMETERS\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "param_grid={'learning_rate': [0.25,0.1,0.05,0.02,0.01],\n",
    "            'max_depth': [3,4,5,6],\n",
    "            'min_samples_leaf':[3,5,9,13,19],\n",
    "            'max_features':[1.0,0.3,0.1]\n",
    "           }\n",
    "est=ensemble.GradientBoostingRegressor(n_estimators=200)\n",
    "gs_cv=GridSearchCV(est,param_grid,scoring='neg_mean_absolute_error',cv=5)\n",
    "gs_cv.fit(valid_low_x,valid_low_y.A1)\n",
    "gs_cv.best_params_,gs_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: -0.19511, std: 0.00220, params: {'n_estimators': 100},\n",
       "  mean: -0.19496, std: 0.00215, params: {'n_estimators': 120},\n",
       "  mean: -0.19483, std: 0.00217, params: {'n_estimators': 140},\n",
       "  mean: -0.19486, std: 0.00218, params: {'n_estimators': 160},\n",
       "  mean: -0.19489, std: 0.00226, params: {'n_estimators': 180},\n",
       "  mean: -0.19484, std: 0.00226, params: {'n_estimators': 200},\n",
       "  mean: -0.19486, std: 0.00226, params: {'n_estimators': 220},\n",
       "  mean: -0.19492, std: 0.00222, params: {'n_estimators': 240},\n",
       "  mean: -0.19492, std: 0.00217, params: {'n_estimators': 260},\n",
       "  mean: -0.19493, std: 0.00215, params: {'n_estimators': 280},\n",
       "  mean: -0.19495, std: 0.00211, params: {'n_estimators': 300}],\n",
       " {'n_estimators': 140},\n",
       " -0.19483087167319926)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test1 = {'n_estimators':[100,120,140,160,180,200,220,240,260,280,300]}\n",
    "gsearch1 = GridSearchCV(estimator = ensemble.GradientBoostingRegressor(learning_rate=0.05,max_features=0.3,min_samples_split=100,min_samples_leaf=9,max_depth=6,subsample=0.8,random_state=10), \n",
    "param_grid = param_test1, scoring='neg_mean_absolute_error',n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(valid_low_x,valid_low_y.A1)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ElasticNet Regressor performed good with high votes\n",
    "# For Elastic Net we can use ElasticNetCV to get better results\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "predictor_high = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1])\n",
    "predictor_high.fit(train_high_x,train_high_y.A1)\n",
    "predict_high_y = predictor_high.predict(valid_high_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gradientboosting\n",
    "#params = {n_estimators= 220, max_depth= 4, min_samples_split= 2, loss= 'ls'}\n",
    "predictor_low = ensemble.GradientBoostingRegressor(n_estimators= 240, max_depth= 4, min_samples_split= 100,learning_rate=0.05)\n",
    "predictor_low.fit(train_low_x,train_low_y.A1)\n",
    "predict_low_y = predictor_low.predict(valid_low_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error of Predictor : 0.0749424255468\n"
     ]
    }
   ],
   "source": [
    "# Mean Absolute Error for Elasticnet\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "mae_high = mae(valid_high_y, predict_high_y)\n",
    "print (\"Mean Absolute Error of Predictor : \" + str(mae_high))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error of Predictor : 0.195411163339\n"
     ]
    }
   ],
   "source": [
    "# Mean Absolute Error for GradientBoost\n",
    "mae_low = mae(valid_low_y, predict_low_y)\n",
    "print (\"Mean Absolute Error of Predictor : \" + str(mae_low))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now using high performing params on the full training dataset to train the model again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(alphas=None, copy_X=True, cv=5, eps=0.001, fit_intercept=True,\n",
       "       l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1], max_iter=1000,\n",
       "       n_alphas=100, n_jobs=1, normalize=False, positive=False,\n",
       "       precompute='auto', random_state=None, selection='cyclic',\n",
       "       tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_high = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1],cv=5)\n",
    "predictor_high.fit(train_high_dataset,train_high_helpfulness.A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.05, loss='ls', max_depth=4, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=1, min_samples_split=100,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=240,\n",
       "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_low = ensemble.GradientBoostingRegressor(n_estimators= 240, max_depth= 4, min_samples_split= 100,learning_rate=0.05)\n",
    "predictor_low.fit(train_low_dataset,train_low_helpfulness.A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_test_samples(predictor, test_data):\n",
    "    return predictor.predict(np.matrix(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Getting the test samples\n",
    "test_dataset = []\n",
    "for line in read_gzip(\"test_Helpful.json.gz\"):\n",
    "    test_dataset.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Features are successfully extracted...\n",
      "(14000, 13)\n"
     ]
    }
   ],
   "source": [
    "#Collecting features for test samples\n",
    "test_feature_set = collect_features(test_dataset)\n",
    "print (test_feature_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing results for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = open(\"predictions5_Helpful.txt\", 'w')\n",
    "index = 0\n",
    "for l in open(\"pairs_Helpful.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        \n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,i,outOf = l.strip().split('-')\n",
    "    outOf = int(outOf)\n",
    "    if outOf==0:\n",
    "        pred=0\n",
    "    if (outOf > 15):\n",
    "        pred = int(np.around(outOf*predict_test_samples(predictor_high, test_feature_set[index])))\n",
    "    else:\n",
    "        pred = int(np.around(outOf*predict_test_samples(predictor_low, test_feature_set[index])))\n",
    "    predictions.write(u + '-' + i + '-' + str(outOf) + ',' + str(pred) + '\\n')\n",
    "    index += 1\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
